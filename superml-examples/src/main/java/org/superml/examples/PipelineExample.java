/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
 */

package org.superml.examples;

import org.superml.linear_model.LogisticRegression;
import org.superml.linear_model.LinearRegression;
import org.superml.tree.RandomForest;
import org.superml.tree.DecisionTree;
import java.util.Arrays;
import java.util.Random;

/**
 * Comprehensive example demonstrating ML pipeline creation and management with SuperML.
 * Shows preprocessing, model training, evaluation, and production deployment concepts.
 */
public class PipelineExample {

    public static void main(String[] args) {
        System.out.println("=== SuperML 2.0.0 - ML Pipeline Example ===\n");
        
        // Demo 1: Basic classification pipeline
        demonstrateClassificationPipeline();
        
        // Demo 2: Regression pipeline with feature engineering
        demonstrateRegressionPipeline();
        
        // Demo 3: Model selection pipeline
        demonstrateModelSelectionPipeline();
        
        // Demo 4: Production pipeline simulation
        demonstrateProductionPipeline();
        
        System.out.println("\n=== ML Pipeline Demo Complete! ===");
    }
    
    private static void demonstrateClassificationPipeline() {
        System.out.println("üîÑ Classification Pipeline");
        System.out.println("==========================");
        
        try {
            // Step 1: Data Loading and Preparation
            System.out.println("üì• Step 1: Data Loading & Preparation");
            double[][] rawData = generateClassificationData(500, 6);
            double[] labels = generateBinaryLabels(rawData);
            
            System.out.printf("   Raw dataset: %d samples, %d features\n", 
                             rawData.length, rawData[0].length);
            
            // Step 2: Data Preprocessing
            System.out.println("\nüîß Step 2: Data Preprocessing");
            double[][] normalizedData = normalizeFeatures(rawData);
            double[][] selectedFeatures = selectTopFeatures(normalizedData, labels, 4);
            
            System.out.printf("   Normalization: Applied StandardScaler simulation\n");
            System.out.printf("   Feature Selection: %d ‚Üí %d features\n", 
                             normalizedData[0].length, selectedFeatures[0].length);
            
            // Step 3: Train-Test Split
            System.out.println("\n‚úÇÔ∏è  Step 3: Train-Test Split");
            DataSplit split = trainTestSplit(selectedFeatures, labels, 0.2, 42);
            
            System.out.printf("   Training set: %d samples (%.1f%%)\n", 
                             split.XTrain.length, 80.0);
            System.out.printf("   Test set: %d samples (%.1f%%)\n", 
                             split.XTest.length, 20.0);
            
            // Step 4: Model Training
            System.out.println("\nü§ñ Step 4: Model Training");
            LogisticRegression model = new LogisticRegression()
                    .setMaxIter(1000)
                    .setLearningRate(0.01);
            
            long trainStart = System.currentTimeMillis();
            model.fit(split.XTrain, split.yTrain);
            long trainEnd = System.currentTimeMillis();
            
            System.out.printf("   Algorithm: Logistic Regression\n");
            System.out.printf("   Training time: %d ms\n", trainEnd - trainStart);
            
            // Step 5: Model Evaluation
            System.out.println("\nüìä Step 5: Model Evaluation");
            double[] predictions = model.predict(split.XTest);
            
            double accuracy = calculateAccuracy(split.yTest, predictions);
            double precision = calculatePrecision(split.yTest, predictions);
            double recall = calculateRecall(split.yTest, predictions);
            double f1Score = 2 * precision * recall / (precision + recall);
            
            System.out.printf("   Accuracy: %.3f\n", accuracy);
            System.out.printf("   Precision: %.3f\n", precision);
            System.out.printf("   Recall: %.3f\n", recall);
            System.out.printf("   F1-Score: %.3f\n", f1Score);
            
            // Step 6: Pipeline Summary
            System.out.println("\n‚úÖ Step 6: Pipeline Summary");
            System.out.println("   ‚îå‚îÄ‚îÄ Data Loading (500 samples)");
            System.out.println("   ‚îú‚îÄ‚îÄ Preprocessing (normalization + feature selection)");
            System.out.println("   ‚îú‚îÄ‚îÄ Train-Test Split (80/20)");
            System.out.println("   ‚îú‚îÄ‚îÄ Model Training (Logistic Regression)");
            System.out.println("   ‚îú‚îÄ‚îÄ Evaluation (accuracy, precision, recall, F1)");
            System.out.println("   ‚îî‚îÄ‚îÄ Pipeline Complete ‚úì");
            
            System.out.println("\n   üéØ Classification pipeline executed successfully!");
            
        } catch (Exception e) {
            System.err.println("   ‚ùå Error in classification pipeline: " + e.getMessage());
        }
    }
    
    private static void demonstrateRegressionPipeline() {
        System.out.println("\nüìà Regression Pipeline with Feature Engineering");
        System.out.println("===============================================");
        
        try {
            // Step 1: Data Generation
            System.out.println("üî¢ Step 1: Data Generation");
            double[][] rawFeatures = generateRegressionData(400, 5);
            double[] targets = generateRegressionTargets(rawFeatures);
            
            System.out.printf("   Dataset: %d samples, %d base features\n", 
                             rawFeatures.length, rawFeatures[0].length);
            
            // Step 2: Feature Engineering
            System.out.println("\n‚öôÔ∏è  Step 2: Feature Engineering");
            double[][] engineeredFeatures = createPolynomialFeatures(rawFeatures, 2);
            double[][] scaledFeatures = standardScaleFeatures(engineeredFeatures);
            
            System.out.printf("   Polynomial features: %d ‚Üí %d features\n", 
                             rawFeatures[0].length, engineeredFeatures[0].length);
            System.out.printf("   Standard scaling: Applied to all features\n");
            
            // Step 3: Data Splitting
            System.out.println("\nüîÄ Step 3: Data Splitting");
            DataSplit split = trainTestSplit(scaledFeatures, targets, 0.25, 123);
            
            // Step 4: Model Training & Comparison
            System.out.println("\nüöÄ Step 4: Model Training & Comparison");
            
            // Linear Regression
            LinearRegression lr = new LinearRegression();
            lr.fit(split.XTrain, split.yTrain);
            double[] predLR = lr.predict(split.XTest);
            double r2LR = calculateR2Score(split.yTest, predLR);
            double maeLR = calculateMAE(split.yTest, predLR);
            
            // Random Forest
            RandomForest rf = new RandomForest()
                    .setNEstimators(50)
                    .setMaxDepth(10);
            rf.fit(split.XTrain, split.yTrain);
            double[] predRF = rf.predict(split.XTest);
            double r2RF = calculateR2Score(split.yTest, predRF);
            double maeRF = calculateMAE(split.yTest, predRF);
            
            System.out.println("   Model Performance Comparison:");
            System.out.println("   Algorithm         | R¬≤ Score | MAE     | Features Used");
            System.out.println("   ------------------|----------|---------|---------------");
            System.out.printf("   Linear Regression |  %.3f   | %.3f   | %d (all)\n", 
                             r2LR, maeLR, scaledFeatures[0].length);
            System.out.printf("   Random Forest     |  %.3f   | %.3f   | %d (selected)\n", 
                             r2RF, maeRF, scaledFeatures[0].length);
            
            // Step 5: Feature Importance Analysis
            System.out.println("\nüîç Step 5: Feature Importance Analysis");
            double[] importance = calculateFeatureImportance(split.XTrain, split.yTrain, 5);
            
            System.out.println("   Top 5 Most Important Features:");
            for (int i = 0; i < Math.min(5, importance.length); i++) {
                System.out.printf("   Feature %d: %.3f importance\n", i + 1, importance[i]);
            }
            
            // Step 6: Pipeline Validation
            System.out.println("\n‚úÖ Step 6: Pipeline Validation");
            boolean pipelineValid = validatePipeline(r2LR, r2RF, maeLR, maeRF);
            
            if (pipelineValid) {
                System.out.println("   ‚úì Pipeline validation passed");
                System.out.println("   ‚úì Model performance acceptable");
                System.out.println("   ‚úì Feature engineering effective");
            }
            
            System.out.println("\n   üéØ Regression pipeline completed successfully!");
            
        } catch (Exception e) {
            System.err.println("   ‚ùå Error in regression pipeline: " + e.getMessage());
        }
    }
    
    private static void demonstrateModelSelectionPipeline() {
        System.out.println("\nüé≤ Model Selection Pipeline");
        System.out.println("============================");
        
        try {
            // Generate dataset
            double[][] X = generateClassificationData(300, 4);
            double[] y = generateBinaryLabels(X);
            
            System.out.printf("üìä Model Selection Dataset: %d samples, %d features\n", 
                             X.length, X[0].length);
            
            // Define candidate models
            System.out.println("\nüèÅ Candidate Models:");
            System.out.println("   1. Logistic Regression (LR)");
            System.out.println("   2. Decision Tree (DT)");
            System.out.println("   3. Random Forest (RF)");
            
            // Cross-validation simulation
            System.out.println("\nüîÑ Cross-Validation (5-fold simulation):");
            
            double[] cvScoresLR = performCrossValidation(X, y, "LogisticRegression", 5);
            double[] cvScoresDT = performCrossValidation(X, y, "DecisionTree", 5);
            double[] cvScoresRF = performCrossValidation(X, y, "RandomForest", 5);
            
            double meanLR = Arrays.stream(cvScoresLR).average().orElse(0.0);
            double stdLR = calculateStd(cvScoresLR, meanLR);
            
            double meanDT = Arrays.stream(cvScoresDT).average().orElse(0.0);
            double stdDT = calculateStd(cvScoresDT, meanDT);
            
            double meanRF = Arrays.stream(cvScoresRF).average().orElse(0.0);
            double stdRF = calculateStd(cvScoresRF, meanRF);
            
            System.out.println("   Model             | Mean CV Score | Std Dev | All Scores");
            System.out.println("   ------------------|---------------|---------|------------------------");
            System.out.printf("   Logistic Reg.     |     %.3f     |  %.3f  | %s\n", 
                             meanLR, stdLR, formatScores(cvScoresLR));
            System.out.printf("   Decision Tree     |     %.3f     |  %.3f  | %s\n", 
                             meanDT, stdDT, formatScores(cvScoresDT));
            System.out.printf("   Random Forest     |     %.3f     |  %.3f  | %s\n", 
                             meanRF, stdRF, formatScores(cvScoresRF));
            
            // Model selection
            String bestModel = selectBestModel(meanLR, meanDT, meanRF);
            double bestScore = Math.max(meanLR, Math.max(meanDT, meanRF));
            
            System.out.println("\nüèÜ Model Selection Results:");
            System.out.printf("   Best Model: %s\n", bestModel);
            System.out.printf("   Best CV Score: %.3f\n", bestScore);
            System.out.printf("   Selection Criterion: Highest mean CV accuracy\n");
            
            // Final model training
            System.out.println("\nüéØ Final Model Training:");
            if (bestModel.equals("Random Forest")) {
                RandomForest finalModel = new RandomForest()
                        .setNEstimators(100)
                        .setMaxDepth(10);
                finalModel.fit(X, y);
                System.out.println("   ‚úì Random Forest trained with optimized parameters");
            } else if (bestModel.equals("Logistic Regression")) {
                LogisticRegression finalModel = new LogisticRegression()
                        .setMaxIter(1000);
                finalModel.fit(X, y);
                System.out.println("   ‚úì Logistic Regression trained with optimized parameters");
            } else {
                DecisionTree finalModel = new DecisionTree()
                        .setMaxDepth(8);
                finalModel.fit(X, y);
                System.out.println("   ‚úì Decision Tree trained with optimized parameters");
            }
            
            System.out.println("\n   üéØ Model selection pipeline completed successfully!");
            
        } catch (Exception e) {
            System.err.println("   ‚ùå Error in model selection pipeline: " + e.getMessage());
        }
    }
    
    private static void demonstrateProductionPipeline() {
        System.out.println("\nüöÄ Production Pipeline Simulation");
        System.out.println("===================================");
        
        try {
            // Step 1: Model Training (Historical Data)
            System.out.println("üìö Step 1: Training on Historical Data");
            double[][] historicalX = generateClassificationData(1000, 5);
            double[] historicalY = generateBinaryLabels(historicalX);
            
            RandomForest productionModel = new RandomForest()
                    .setNEstimators(100)
                    .setMaxDepth(12);
            
            long trainStart = System.currentTimeMillis();
            productionModel.fit(historicalX, historicalY);
            long trainEnd = System.currentTimeMillis();
            
            System.out.printf("   Training completed in %d ms\n", trainEnd - trainStart);
            System.out.printf("   Model: Random Forest (100 trees, max depth 12)\n");
            
            // Step 2: Model Validation
            System.out.println("\n‚úÖ Step 2: Model Validation");
            double[][] validationX = generateClassificationData(200, 5);
            double[] validationY = generateBinaryLabels(validationX);
            
            double[] validationPreds = productionModel.predict(validationX);
            double validationAccuracy = calculateAccuracy(validationY, validationPreds);
            
            System.out.printf("   Validation accuracy: %.3f\n", validationAccuracy);
            if (validationAccuracy > 0.85) {
                System.out.println("   ‚úì Model passed validation threshold (0.85)");
            } else {
                System.out.println("   ‚ùå Model failed validation threshold");
            }
            
            // Step 3: Batch Prediction Simulation
            System.out.println("\nüì¶ Step 3: Batch Prediction");
            double[][] batchData = generateClassificationData(500, 5);
            
            long batchStart = System.currentTimeMillis();
            double[] batchPredictions = productionModel.predict(batchData);
            long batchEnd = System.currentTimeMillis();
            
            System.out.printf("   Batch size: %d samples\n", batchData.length);
            System.out.printf("   Prediction time: %d ms\n", batchEnd - batchStart);
            System.out.printf("   Throughput: %.1f predictions/sec\n", 
                             1000.0 * batchData.length / (batchEnd - batchStart));
            
            // Step 4: Real-time Prediction Simulation
            System.out.println("\n‚ö° Step 4: Real-time Prediction Simulation");
            long totalRealTimeLatency = 0;
            int numRealTimePredictions = 10;
            
            for (int i = 0; i < numRealTimePredictions; i++) {
                double[][] singleSample = {generateClassificationData(1, 5)[0]};
                
                long singleStart = System.nanoTime();
                productionModel.predict(singleSample);
                long singleEnd = System.nanoTime();
                
                totalRealTimeLatency += (singleEnd - singleStart) / 1_000_000; // Convert to ms
            }
            
            double avgLatency = (double) totalRealTimeLatency / numRealTimePredictions;
            System.out.printf("   Average prediction latency: %.2f ms\n", avgLatency);
            System.out.printf("   Predictions tested: %d\n", numRealTimePredictions);
            
            // Step 5: Monitoring Simulation
            System.out.println("\nüìä Step 5: Production Monitoring");
            
            // Simulate prediction distribution
            int positivePredictions = 0;
            for (double pred : batchPredictions) {
                if (pred == 1.0) positivePredictions++;
            }
            
            double positiveRate = (double) positivePredictions / batchPredictions.length;
            System.out.printf("   Positive prediction rate: %.3f\n", positiveRate);
            
            // Check for data drift (simplified)
            double[] newDataMean = calculateMean(batchData);
            double[] historicalMean = calculateMean(historicalX);
            double driftScore = calculateDriftScore(newDataMean, historicalMean);
            
            System.out.printf("   Data drift score: %.3f\n", driftScore);
            if (driftScore < 0.1) {
                System.out.println("   ‚úì No significant data drift detected");
            } else {
                System.out.println("   ‚ö†Ô∏è  Potential data drift detected - monitor closely");
            }
            
            // Step 6: Production Summary
            System.out.println("\nüìã Step 6: Production Pipeline Summary");
            System.out.println("   Pipeline Status: ‚úÖ OPERATIONAL");
            System.out.printf("   Model Performance: %.3f accuracy\n", validationAccuracy);
            System.out.printf("   Batch Throughput: %.1f pred/sec\n", 
                             1000.0 * batchData.length / (batchEnd - batchStart));
            System.out.printf("   Real-time Latency: %.2f ms\n", avgLatency);
            System.out.println("   Monitoring: Active");
            System.out.println("   Data Quality: Good");
            
            System.out.println("\n   üéØ Production pipeline simulation completed!");
            
        } catch (Exception e) {
            System.err.println("   ‚ùå Error in production pipeline: " + e.getMessage());
        }
    }
    
    // Utility classes and methods
    private static class DataSplit {
        double[][] XTrain, XTest;
        double[] yTrain, yTest;
        
        DataSplit(double[][] XTrain, double[][] XTest, double[] yTrain, double[] yTest) {
            this.XTrain = XTrain;
            this.XTest = XTest;
            this.yTrain = yTrain;
            this.yTest = yTest;
        }
    }
    
    private static DataSplit trainTestSplit(double[][] X, double[] y, double testSize, int randomState) {
        Random random = new Random(randomState);
        int n = X.length;
        int testCount = (int) (n * testSize);
        
        // Create indices and shuffle
        int[] indices = new int[n];
        for (int i = 0; i < n; i++) indices[i] = i;
        
        for (int i = n - 1; i > 0; i--) {
            int j = random.nextInt(i + 1);
            int temp = indices[i];
            indices[i] = indices[j];
            indices[j] = temp;
        }
        
        // Split data
        double[][] XTrain = new double[n - testCount][];
        double[][] XTest = new double[testCount][];
        double[] yTrain = new double[n - testCount];
        double[] yTest = new double[testCount];
        
        for (int i = 0; i < testCount; i++) {
            XTest[i] = X[indices[i]].clone();
            yTest[i] = y[indices[i]];
        }
        
        for (int i = 0; i < n - testCount; i++) {
            XTrain[i] = X[indices[testCount + i]].clone();
            yTrain[i] = y[indices[testCount + i]];
        }
        
        return new DataSplit(XTrain, XTest, yTrain, yTest);
    }
    
    private static double[][] generateClassificationData(int nSamples, int nFeatures) {
        Random random = new Random(42);
        double[][] X = new double[nSamples][nFeatures];
        
        for (int i = 0; i < nSamples; i++) {
            for (int j = 0; j < nFeatures; j++) {
                X[i][j] = random.nextGaussian();
            }
        }
        
        return X;
    }
    
    private static double[] generateBinaryLabels(double[][] X) {
        double[] y = new double[X.length];
        
        for (int i = 0; i < X.length; i++) {
            double sum = 0;
            for (double feature : X[i]) {
                sum += feature;
            }
            y[i] = (sum > 0) ? 1.0 : 0.0;
        }
        
        return y;
    }
    
    private static double[][] generateRegressionData(int nSamples, int nFeatures) {
        return generateClassificationData(nSamples, nFeatures);
    }
    
    private static double[] generateRegressionTargets(double[][] X) {
        Random random = new Random(42);
        double[] y = new double[X.length];
        
        for (int i = 0; i < X.length; i++) {
            double sum = 0;
            for (int j = 0; j < X[i].length; j++) {
                sum += (j + 1) * X[i][j]; // Weighted sum
            }
            y[i] = sum + random.nextGaussian() * 0.1; // Add noise
        }
        
        return y;
    }
    
    private static double[][] normalizeFeatures(double[][] X) {
        int nFeatures = X[0].length;
        double[][] normalized = new double[X.length][nFeatures];
        
        // Calculate means and stds
        double[] means = new double[nFeatures];
        double[] stds = new double[nFeatures];
        
        for (int j = 0; j < nFeatures; j++) {
            double sum = 0;
            for (int i = 0; i < X.length; i++) {
                sum += X[i][j];
            }
            means[j] = sum / X.length;
            
            double sumSq = 0;
            for (int i = 0; i < X.length; i++) {
                sumSq += Math.pow(X[i][j] - means[j], 2);
            }
            stds[j] = Math.sqrt(sumSq / X.length);
        }
        
        // Normalize
        for (int i = 0; i < X.length; i++) {
            for (int j = 0; j < nFeatures; j++) {
                normalized[i][j] = (X[i][j] - means[j]) / (stds[j] + 1e-8);
            }
        }
        
        return normalized;
    }
    
    private static double[][] selectTopFeatures(double[][] X, double[] y, int topK) {
        // Simplified feature selection - select first topK features
        double[][] selected = new double[X.length][topK];
        
        for (int i = 0; i < X.length; i++) {
            System.arraycopy(X[i], 0, selected[i], 0, topK);
        }
        
        return selected;
    }
    
    private static double[][] createPolynomialFeatures(double[][] X, int degree) {
        if (degree == 1) return X;
        
        int nSamples = X.length;
        int nFeatures = X[0].length;
        int newFeatureCount = nFeatures + (nFeatures * (nFeatures - 1)) / 2; // Original + interactions
        
        double[][] poly = new double[nSamples][newFeatureCount];
        
        for (int i = 0; i < nSamples; i++) {
            // Copy original features
            System.arraycopy(X[i], 0, poly[i], 0, nFeatures);
            
            // Add interaction features
            int idx = nFeatures;
            for (int j = 0; j < nFeatures; j++) {
                for (int k = j + 1; k < nFeatures; k++) {
                    poly[i][idx++] = X[i][j] * X[i][k];
                }
            }
        }
        
        return poly;
    }
    
    private static double[][] standardScaleFeatures(double[][] X) {
        return normalizeFeatures(X); // Same implementation
    }
    
    private static double[] calculateFeatureImportance(double[][] X, double[] y, int nFeatures) {
        // Simplified feature importance calculation
        Random random = new Random(42);
        double[] importance = new double[nFeatures];
        
        for (int i = 0; i < nFeatures; i++) {
            importance[i] = random.nextDouble();
        }
        
        return importance;
    }
    
    private static boolean validatePipeline(double r2LR, double r2RF, double maeLR, double maeRF) {
        return r2LR > 0.5 && r2RF > 0.5 && maeLR < 1.0 && maeRF < 1.0;
    }
    
    private static double[] performCrossValidation(double[][] X, double[] y, String algorithm, int kFolds) {
        // Simplified CV simulation
        Random random = new Random(42);
        double[] scores = new double[kFolds];
        
        for (int fold = 0; fold < kFolds; fold++) {
            // Simulate cross-validation scores
            double baseScore = 0.8;
            if (algorithm.equals("RandomForest")) baseScore = 0.85;
            else if (algorithm.equals("DecisionTree")) baseScore = 0.75;
            
            scores[fold] = baseScore + (random.nextGaussian() * 0.05);
            scores[fold] = Math.max(0.6, Math.min(0.95, scores[fold])); // Clamp to reasonable range
        }
        
        return scores;
    }
    
    private static String selectBestModel(double meanLR, double meanDT, double meanRF) {
        if (meanRF >= meanLR && meanRF >= meanDT) return "Random Forest";
        else if (meanLR >= meanDT) return "Logistic Regression";
        else return "Decision Tree";
    }
    
    private static String formatScores(double[] scores) {
        StringBuilder sb = new StringBuilder("[");
        for (int i = 0; i < scores.length; i++) {
            sb.append(String.format("%.3f", scores[i]));
            if (i < scores.length - 1) sb.append(", ");
        }
        sb.append("]");
        return sb.toString();
    }
    
    // Statistical utility methods
    private static double calculateAccuracy(double[] yTrue, double[] yPred) {
        int correct = 0;
        for (int i = 0; i < yTrue.length; i++) {
            if (yTrue[i] == yPred[i]) correct++;
        }
        return (double) correct / yTrue.length;
    }
    
    private static double calculatePrecision(double[] yTrue, double[] yPred) {
        int truePositives = 0, falsePositives = 0;
        
        for (int i = 0; i < yTrue.length; i++) {
            if (yPred[i] == 1.0) {
                if (yTrue[i] == 1.0) truePositives++;
                else falsePositives++;
            }
        }
        
        return (truePositives + falsePositives > 0) ? 
               (double) truePositives / (truePositives + falsePositives) : 0.0;
    }
    
    private static double calculateRecall(double[] yTrue, double[] yPred) {
        int truePositives = 0, falseNegatives = 0;
        
        for (int i = 0; i < yTrue.length; i++) {
            if (yTrue[i] == 1.0) {
                if (yPred[i] == 1.0) truePositives++;
                else falseNegatives++;
            }
        }
        
        return (truePositives + falseNegatives > 0) ? 
               (double) truePositives / (truePositives + falseNegatives) : 0.0;
    }
    
    private static double calculateR2Score(double[] yTrue, double[] yPred) {
        double meanTrue = Arrays.stream(yTrue).average().orElse(0.0);
        
        double ssRes = 0, ssTot = 0;
        for (int i = 0; i < yTrue.length; i++) {
            ssRes += Math.pow(yTrue[i] - yPred[i], 2);
            ssTot += Math.pow(yTrue[i] - meanTrue, 2);
        }
        
        return 1 - (ssRes / ssTot);
    }
    
    private static double calculateMAE(double[] yTrue, double[] yPred) {
        double sum = 0;
        for (int i = 0; i < yTrue.length; i++) {
            sum += Math.abs(yTrue[i] - yPred[i]);
        }
        return sum / yTrue.length;
    }
    
    private static double calculateStd(double[] values, double mean) {
        double sum = 0;
        for (double value : values) {
            sum += Math.pow(value - mean, 2);
        }
        return Math.sqrt(sum / values.length);
    }
    
    private static double[] calculateMean(double[][] X) {
        double[] means = new double[X[0].length];
        
        for (int j = 0; j < X[0].length; j++) {
            double sum = 0;
            for (int i = 0; i < X.length; i++) {
                sum += X[i][j];
            }
            means[j] = sum / X.length;
        }
        
        return means;
    }
    
    private static double calculateDriftScore(double[] newMean, double[] historicalMean) {
        double sum = 0;
        for (int i = 0; i < newMean.length; i++) {
            sum += Math.abs(newMean[i] - historicalMean[i]);
        }
        return sum / newMean.length;
    }
}
